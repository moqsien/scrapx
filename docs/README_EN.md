### what's scrapxï¼Ÿ

An efficient framework based on Scrapy for extracting the data you need from websites.

It has has the following characteristics:

- more reasonable directory structure design for a big project contains maybe thousands of spiders.
    - first-level directory is workspace for all projects.
    - second-level directory is project dir for spiders that are classified by 'data type'.
    - third-level directory is the same as 'spiders' in scrapy project.

- more elegant since some redundant files in a scrapy project are removed.(see example).
    - settings.py, middlewares.py, pipelines.py moved to 'scrapx_globals' in workspace, which you can load them in all project to avoid repitition.
    - items.py is merged to spider file in spiders.
    - run_xx.py is generated by templates which contains Unique configuration for each spider. It can be executed as a script while debuging your spider.
    - MongoDB is introduced for storing data and spider info.
- Customized cmds for scrapx.
    - initiate, generate a workspace
    - genproject, generate a project
    - genspider, generate a spider and its run file.
- All parameters supported in scrapy's settings.py are accepted by scrapx.

### how to use?
- install scrapx
```bash
# from source
git clone git@github.com:moqsien/scrapx.git
cd scrapx
python setup.py build
python setup.py install
# or
# through pip
pip install scrapx
```

- initiate a workspace
```bash
scrapx -h # help info
scrapx initiate example
```

- create a project
```bash
cd example
scrapx genproject test1
```

- generate a spider
```bash
cd test1
scrapx genspider $spider_name $domain
```

- install mongodb
```bash
# if you already have mongodb installed, 
# change the settings info in scrapx_globals
docker pull mongo:latest

docker run -itd --name mongo -p 27017:27017 -v ~/mongodb:/data/db mongo --auth

docker exec -it mongo mongo admin

# authorization configure
db.createUser({ user:'admin',pwd:'654321',roles:[ { role:'userAdminAnyDatabase', db: 'admin'}]});
db.auth("admin", "654321")
db.grantRolesToUser("admin", [ { role: "readWrite", db: "crawler" } ])
db.grantRolesToUser("admin", [ { role: "readWrite", db: "crawler_statistic" } ])
```

- compose a spider and modify settings in its run_xxx.py file.

- run a spider
```bash
scrapx crawl tzrb # in a project
# or
scrapx crawl test1.tzrb # in a workspace
# or
# you can just run the run_xxx.py as a script
```

- What's the mongodb collection like if you have already successfully run the example?
![avatar](https://github.com/moqsien/scrapx/blob/main/docs/mongodb.png)

### TODO
- [ ] a customized version of scrapyd to support scrapx
- [ ] proxy pool
- [ ] log collection and analyzation
- [ ] error spying and visulization

### Acknowledgment
Thanks to scrapy contributors and some of my friends.


### Statement
This project is created for individual study, that means any part of its functions will have no guarantee, and the author will take no responsibility for any of legal risks in apply this projcet for other use.
